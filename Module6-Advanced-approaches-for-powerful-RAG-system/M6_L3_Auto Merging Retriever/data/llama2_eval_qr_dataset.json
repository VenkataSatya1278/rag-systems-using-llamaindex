{
    "examples": [
        {
            "query": "What are the key differences between the pretraining and fine-tuning processes described for Llama 2, and how do these processes contribute to the model's performance in dialogue use cases?",
            "query_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            },
            "reference_contexts": [
                "Llama 2: Open Foundation and Fine-Tuned Chat Models\nHugo Touvron\u2217\nLouis Martin\u2020\nKevin Stone\u2020\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\nSergey Edunov\nThomas Scialom\u2217\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\nsource models. We provide a detailed description of our approach to fine-tuning and safety\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.\n\u2217Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n\u2020Second author\nContributions for all the authors can be found in Section A.1.\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n\n\nContents\n1\nIntroduction\n3\n2\nPretraining\n5\n2.1\nPretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nTraining Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.3\nLlama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3\nFine-tuning\n8\n3.1\nSupervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.2\nReinforcement Learning with Human Feedback (RLHF)\n. . . . . . . . . . . . . . . . . . . . .\n9\n3.3\nSystem Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.4\nRLHF Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4\nSafety\n20\n4.1\nSafety in Pretraining\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.2\nSafety Fine-Tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.3\nRed Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.4\nSafety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5\nDiscussion\n32\n5.1\nLearnings and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5.2\nLimitations and Ethical Considerations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.3\nResponsible Release Strategy\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n6\nRelated Work\n35\n7\nConclusion\n36\nA Appendix\n46\nA.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nA.2 Additional Details for Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nA.3 Additional Details for Fine-tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\nA.4 Additional Details for Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\nA.5 Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nA.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\nA.7 Model Card\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n2"
            ],
            "reference_answer": "The key differences between the pretraining and fine-tuning processes for Llama 2, as described in the context, are as follows:\n\n1. **Pretraining**:\n   - **Objective**: The pretraining process involves training the model on a large corpus of text data to learn general language patterns and representations. This phase is crucial for establishing a foundational understanding of language.\n   - **Data**: Pretraining uses a diverse and extensive dataset to expose the model to a wide range of language structures and topics.\n   - **Evaluation**: The pretrained model is evaluated to ensure it has learned useful language representations, but it is not yet optimized for specific tasks like dialogue.\n\n2. **Fine-tuning**:\n   - **Objective**: Fine-tuning is the process of adapting the pretrained model to specific tasks or use cases, such as dialogue. This involves further training the model on task-specific data to improve its performance in those areas.\n   - **Methods**: The fine-tuning process for Llama 2 includes Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF). SFT involves training the model on labeled data, while RLHF uses feedback from human evaluators to refine the model's responses.\n   - **System Message for Multi-Turn Consistency**: This aspect of fine-tuning ensures that the model maintains consistency across multiple turns in a conversation, which is crucial for dialogue applications.\n\n**Contribution to Performance in Dialogue Use Cases**:\n- The pretraining phase provides the model with a broad understanding of language, which is essential for generating coherent and contextually relevant responses.\n- The fine-tuning phase tailors the model to handle dialogue-specific challenges, such as maintaining context over multiple turns and generating responses that are both helpful and safe.\n- By using techniques like RLHF, the model is further refined to align with human expectations and preferences, enhancing its ability to perform well in dialogue scenarios.\n\nOverall, the combination of pretraining and fine-tuning allows Llama 2 to leverage a strong language foundation while being specifically optimized for dialogue use cases, resulting in improved performance compared to models that are not fine-tuned for such tasks.",
            "reference_answer_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            }
        },
        {
            "query": "Explain the role of Reinforcement Learning with Human Feedback (RLHF) in the fine-tuning of Llama 2-Chat models and discuss its impact on the model's helpfulness and safety.",
            "query_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            },
            "reference_contexts": [
                "Llama 2: Open Foundation and Fine-Tuned Chat Models\nHugo Touvron\u2217\nLouis Martin\u2020\nKevin Stone\u2020\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\nSergey Edunov\nThomas Scialom\u2217\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\nsource models. We provide a detailed description of our approach to fine-tuning and safety\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.\n\u2217Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n\u2020Second author\nContributions for all the authors can be found in Section A.1.\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n\n\nContents\n1\nIntroduction\n3\n2\nPretraining\n5\n2.1\nPretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nTraining Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.3\nLlama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3\nFine-tuning\n8\n3.1\nSupervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.2\nReinforcement Learning with Human Feedback (RLHF)\n. . . . . . . . . . . . . . . . . . . . .\n9\n3.3\nSystem Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.4\nRLHF Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4\nSafety\n20\n4.1\nSafety in Pretraining\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.2\nSafety Fine-Tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.3\nRed Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.4\nSafety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5\nDiscussion\n32\n5.1\nLearnings and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5.2\nLimitations and Ethical Considerations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.3\nResponsible Release Strategy\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n6\nRelated Work\n35\n7\nConclusion\n36\nA Appendix\n46\nA.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nA.2 Additional Details for Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nA.3 Additional Details for Fine-tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\nA.4 Additional Details for Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\nA.5 Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nA.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\nA.7 Model Card\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n2"
            ],
            "reference_answer": "Reinforcement Learning with Human Feedback (RLHF) plays a crucial role in the fine-tuning of Llama 2-Chat models. RLHF is a method that involves using human feedback to guide the training process of a model, allowing it to learn from human preferences and improve its performance in specific tasks. In the context of Llama 2-Chat, RLHF is used to optimize the model for dialogue use cases, enhancing its ability to generate responses that are not only contextually appropriate but also align with human expectations for helpfulness and safety.\n\nThe impact of RLHF on the model's helpfulness is significant. By incorporating human feedback, the model can better understand and prioritize responses that users find useful and informative. This process helps the model to generate answers that are more aligned with human-like reasoning and decision-making, ultimately making the interactions more satisfying for users.\n\nIn terms of safety, RLHF contributes to the model's ability to avoid generating harmful or inappropriate content. Human feedback is used to identify and correct potentially unsafe outputs, guiding the model to produce responses that adhere to safety guidelines and ethical considerations. This aspect of RLHF is crucial for ensuring that the model can be deployed in real-world applications without posing risks to users.\n\nOverall, RLHF enhances the Llama 2-Chat models by making them more responsive to human needs and expectations, improving both the quality and safety of the interactions they facilitate.",
            "reference_answer_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            }
        },
        {
            "query": "Describe the safety measures implemented during the pretraining and fine-tuning phases of Llama 2, and evaluate their effectiveness based on the safety evaluation results mentioned in the document.",
            "query_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            },
            "reference_contexts": [
                "Llama 2: Open Foundation and Fine-Tuned Chat Models\nHugo Touvron\u2217\nLouis Martin\u2020\nKevin Stone\u2020\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\nSergey Edunov\nThomas Scialom\u2217\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\nsource models. We provide a detailed description of our approach to fine-tuning and safety\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.\n\u2217Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n\u2020Second author\nContributions for all the authors can be found in Section A.1.\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n\n\nContents\n1\nIntroduction\n3\n2\nPretraining\n5\n2.1\nPretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nTraining Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.3\nLlama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3\nFine-tuning\n8\n3.1\nSupervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.2\nReinforcement Learning with Human Feedback (RLHF)\n. . . . . . . . . . . . . . . . . . . . .\n9\n3.3\nSystem Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.4\nRLHF Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4\nSafety\n20\n4.1\nSafety in Pretraining\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.2\nSafety Fine-Tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.3\nRed Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.4\nSafety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5\nDiscussion\n32\n5.1\nLearnings and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5.2\nLimitations and Ethical Considerations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.3\nResponsible Release Strategy\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n6\nRelated Work\n35\n7\nConclusion\n36\nA Appendix\n46\nA.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nA.2 Additional Details for Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nA.3 Additional Details for Fine-tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\nA.4 Additional Details for Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\nA.5 Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nA.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\nA.7 Model Card\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n2"
            ],
            "reference_answer": "The safety measures implemented during the pretraining and fine-tuning phases of Llama 2 are detailed in the document, focusing on ensuring the model's helpfulness and safety. Here's a summary of these measures and their effectiveness based on the safety evaluation results:\n\n1. **Safety in Pretraining:**\n   - The document outlines that safety considerations were integrated into the pretraining phase. This likely involved curating the pretraining data to minimize harmful content and biases, although specific strategies are not detailed in the provided context.\n\n2. **Safety Fine-Tuning:**\n   - During the fine-tuning phase, specific safety fine-tuning techniques were applied. This phase likely involved using Reinforcement Learning with Human Feedback (RLHF) to align the model's outputs with human values and safety standards. The document mentions a section on \"Safety Fine-Tuning,\" indicating a focused effort to enhance the model's safety through this process.\n\n3. **Red Teaming:**\n   - Red teaming is a process where the model is tested against potential adversarial inputs to identify vulnerabilities. This helps in understanding how the model might behave in unsafe or harmful scenarios and allows for adjustments to improve its robustness.\n\n4. **Safety Evaluation of Llama 2-Chat:**\n   - The document includes a section on the \"Safety Evaluation of Llama 2-Chat,\" which suggests that the effectiveness of the safety measures was assessed. While specific results are not provided in the context, the mention of this evaluation indicates that the model's safety was rigorously tested, and the results likely informed further improvements.\n\nOverall, the document suggests that Llama 2's safety measures were comprehensive, involving both pretraining and fine-tuning phases, and were evaluated through systematic testing. The effectiveness of these measures is implied to be positive, as the models are described as potentially suitable substitutes for closed-source models in terms of safety and helpfulness.",
            "reference_answer_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            }
        },
        {
            "query": "What are some of the key improvements made in Llama 2 compared to its predecessor, Llama 1, in terms of training data and model architecture?",
            "query_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            },
            "reference_contexts": [
                "Figure 1: Helpfulness human evaluation results for Llama\n2-Chat compared to other open-source and closed-source\nmodels. Human raters compared model generations on ~4k\nprompts consisting of both single and multi-turn prompts.\nThe 95% confidence intervals for this evaluation are between\n1% and 2%. More details in Section 3.4.2. While reviewing\nthese results, it is important to note that human evaluations\ncan be noisy due to limitations of the prompt set, subjectivity\nof the review guidelines, subjectivity of individual raters,\nand the inherent difficulty of comparing generations.\nFigure 2: Win-rate % for helpfulness and\nsafety between commercial-licensed base-\nlines and Llama 2-Chat, according to GPT-\n4. To complement the human evaluation, we\nused a more capable model, not subject to\nour own guidance. Green area indicates our\nmodel is better according to GPT-4. To remove\nties, we used win/(win + loss). The orders in\nwhich the model responses are presented to\nGPT-4 are randomly swapped to alleviate bias.\n1\nIntroduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed \u201cproduct\u201d LLMs, such\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety. This step can require significant costs in\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\nthe community to advance AI alignment research.\nIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\nWe also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\nthe emergence of tool usage and temporal organization of knowledge.\n3\n\n\nFigure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-\nsource models. Human raters judged model generations for safety violations across ~2,000 adversarial\nprompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is\nimportant to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the\nprompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these\nsafety evaluations are performed using content standards that are likely to be biased towards the Llama\n2-Chat models.\nWe are releasing the following models to the general public for research and commercial use\u2021:\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.\u00a7\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well.\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not \u2014 and could not \u2014 cover\nall scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\nguide\u00b6 and code examples\u2016 to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\nour responsible release strategy can be found in Section 5.3.\nThe remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\nwork (Section 6), and conclusions (Section 7).\n\u2021https://ai.meta.com/resources/models-and-libraries/llama/\n\u00a7We are delaying the release of the 34B model due to a lack of time to sufficiently red team.\n\u00b6https://ai.meta.com/llama\n\u2016https://github.com/facebookresearch/llama\n4"
            ],
            "reference_answer": "Some of the key improvements made in Llama 2 compared to its predecessor, Llama 1, include:\n\n1. **Training Data**: Llama 2 was trained on a new mix of publicly available data, and the size of the pretraining corpus was increased by 40%.\n\n2. **Model Architecture**: The context length of the model was doubled, and grouped-query attention was adopted.\n\nThese enhancements contribute to the improved performance and capabilities of Llama 2 over Llama 1.",
            "reference_answer_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            }
        },
        {
            "query": "How does the human evaluation process for assessing the helpfulness and safety of Llama 2-Chat account for potential biases and limitations?",
            "query_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            },
            "reference_contexts": [
                "Figure 1: Helpfulness human evaluation results for Llama\n2-Chat compared to other open-source and closed-source\nmodels. Human raters compared model generations on ~4k\nprompts consisting of both single and multi-turn prompts.\nThe 95% confidence intervals for this evaluation are between\n1% and 2%. More details in Section 3.4.2. While reviewing\nthese results, it is important to note that human evaluations\ncan be noisy due to limitations of the prompt set, subjectivity\nof the review guidelines, subjectivity of individual raters,\nand the inherent difficulty of comparing generations.\nFigure 2: Win-rate % for helpfulness and\nsafety between commercial-licensed base-\nlines and Llama 2-Chat, according to GPT-\n4. To complement the human evaluation, we\nused a more capable model, not subject to\nour own guidance. Green area indicates our\nmodel is better according to GPT-4. To remove\nties, we used win/(win + loss). The orders in\nwhich the model responses are presented to\nGPT-4 are randomly swapped to alleviate bias.\n1\nIntroduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed \u201cproduct\u201d LLMs, such\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety. This step can require significant costs in\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\nthe community to advance AI alignment research.\nIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\nWe also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\nthe emergence of tool usage and temporal organization of knowledge.\n3\n\n\nFigure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-\nsource models. Human raters judged model generations for safety violations across ~2,000 adversarial\nprompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is\nimportant to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the\nprompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these\nsafety evaluations are performed using content standards that are likely to be biased towards the Llama\n2-Chat models.\nWe are releasing the following models to the general public for research and commercial use\u2021:\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.\u00a7\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well.\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not \u2014 and could not \u2014 cover\nall scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\nguide\u00b6 and code examples\u2016 to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\nour responsible release strategy can be found in Section 5.3.\nThe remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\nwork (Section 6), and conclusions (Section 7).\n\u2021https://ai.meta.com/resources/models-and-libraries/llama/\n\u00a7We are delaying the release of the 34B model due to a lack of time to sufficiently red team.\n\u00b6https://ai.meta.com/llama\n\u2016https://github.com/facebookresearch/llama\n4"
            ],
            "reference_answer": "The human evaluation process for assessing the helpfulness and safety of Llama 2-Chat accounts for potential biases and limitations in several ways:\n\n1. **Acknowledgment of Noisiness**: The evaluation acknowledges that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, and the subjectivity of individual raters. This recognition helps in understanding the inherent variability in human assessments.\n\n2. **Confidence Intervals**: The results include 95% confidence intervals, which range between 1% and 2%, providing a statistical measure of the reliability of the evaluation results.\n\n3. **Use of a More Capable Model**: To complement human evaluations, a more capable model (GPT-4) is used to assess the win-rate percentage for helpfulness and safety. This model is not subject to the same guidance as human raters, potentially reducing bias.\n\n4. **Random Swapping of Model Responses**: The order in which model responses are presented to GPT-4 is randomly swapped to alleviate bias, ensuring that the evaluation is not influenced by the sequence of presentation.\n\n5. **Content Standards Bias**: The safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models, which is acknowledged as a limitation.\n\nBy incorporating these measures, the evaluation process attempts to mitigate some of the biases and limitations inherent in human assessments.",
            "reference_answer_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            }
        },
        {
            "query": "Describe the methodology and techniques used in fine-tuning Llama 2-Chat to enhance its performance in dialogue use cases.",
            "query_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            },
            "reference_contexts": [
                "Figure 1: Helpfulness human evaluation results for Llama\n2-Chat compared to other open-source and closed-source\nmodels. Human raters compared model generations on ~4k\nprompts consisting of both single and multi-turn prompts.\nThe 95% confidence intervals for this evaluation are between\n1% and 2%. More details in Section 3.4.2. While reviewing\nthese results, it is important to note that human evaluations\ncan be noisy due to limitations of the prompt set, subjectivity\nof the review guidelines, subjectivity of individual raters,\nand the inherent difficulty of comparing generations.\nFigure 2: Win-rate % for helpfulness and\nsafety between commercial-licensed base-\nlines and Llama 2-Chat, according to GPT-\n4. To complement the human evaluation, we\nused a more capable model, not subject to\nour own guidance. Green area indicates our\nmodel is better according to GPT-4. To remove\nties, we used win/(win + loss). The orders in\nwhich the model responses are presented to\nGPT-4 are randomly swapped to alleviate bias.\n1\nIntroduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed \u201cproduct\u201d LLMs, such\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\npreferences, which greatly enhances their usability and safety. This step can require significant costs in\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\nthe community to advance AI alignment research.\nIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\nWe also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\nthe emergence of tool usage and temporal organization of knowledge.\n3\n\n\nFigure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-\nsource models. Human raters judged model generations for safety violations across ~2,000 adversarial\nprompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is\nimportant to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the\nprompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these\nsafety evaluations are performed using content standards that are likely to be biased towards the Llama\n2-Chat models.\nWe are releasing the following models to the general public for research and commercial use\u2021:\n1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.\u00a7\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well.\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not \u2014 and could not \u2014 cover\nall scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\nguide\u00b6 and code examples\u2016 to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\nour responsible release strategy can be found in Section 5.3.\nThe remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\nwork (Section 6), and conclusions (Section 7).\n\u2021https://ai.meta.com/resources/models-and-libraries/llama/\n\u00a7We are delaying the release of the 34B model due to a lack of time to sufficiently red team.\n\u00b6https://ai.meta.com/llama\n\u2016https://github.com/facebookresearch/llama\n4"
            ],
            "reference_answer": "The fine-tuning methodology for Llama 2-Chat involves optimizing the model specifically for dialogue use cases. This process includes aligning the model with human preferences through techniques such as Reinforcement Learning with Human Feedback (RLHF). The fine-tuning process also incorporates safety-specific data annotation and tuning, as well as conducting red-teaming and iterative evaluations to enhance the model's safety and usability. These steps are designed to improve the model's performance in dialogue scenarios, making it more suitable for interaction with humans through chat interfaces. The approach aims to ensure that Llama 2-Chat is both helpful and safe, aligning with human preferences and content standards.",
            "reference_answer_by": {
                "model_name": "gpt-4o",
                "type": "ai"
            }
        }
    ]
}